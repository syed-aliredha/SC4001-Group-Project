{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "108326f3fe5c44489f58d0c3e7fcb73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_1b7d9e322d0d4de8a7f9a8cc83207b5a"
          }
        },
        "6a7b0bb73f6d4ff5b56a318685b8f7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98eeb628db0845a8a791e6346cde2474",
            "placeholder": "​",
            "style": "IPY_MODEL_0b38b3a399c0403d9085861162b3016a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "fa91ba7cfc76465484e50249ce847858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9d44d4964fd647999cb3a54f415be8e2",
            "placeholder": "​",
            "style": "IPY_MODEL_5fea14fdf19a422493ecba7c7b8a0d81",
            "value": ""
          }
        },
        "1fa002466c9c464b88c745745a51fdc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3b7d8cf57f4a4b6e840ffb1d7940ef03",
            "style": "IPY_MODEL_fbc0d876b99e4346b71626e8af51fef3",
            "value": true
          }
        },
        "01dae3ce630b47a89e69ba1c6d28d414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2ec7e75e17b641b7a5117c8ab3e4bcbc",
            "style": "IPY_MODEL_12ac7ec9d4b84ce797c9faffc88bbd1c",
            "tooltip": ""
          }
        },
        "78d50014f7db4d08bafdc02d63b07809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae53229323847d7bc0e21fee0867cfc",
            "placeholder": "​",
            "style": "IPY_MODEL_8a5e3aaf532f46cfb8cad69bdb3c1924",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "1b7d9e322d0d4de8a7f9a8cc83207b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "98eeb628db0845a8a791e6346cde2474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b38b3a399c0403d9085861162b3016a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d44d4964fd647999cb3a54f415be8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fea14fdf19a422493ecba7c7b8a0d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7d8cf57f4a4b6e840ffb1d7940ef03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc0d876b99e4346b71626e8af51fef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ec7e75e17b641b7a5117c8ab3e4bcbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ac7ec9d4b84ce797c9faffc88bbd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7ae53229323847d7bc0e21fee0867cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5e3aaf532f46cfb8cad69bdb3c1924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d71d259271e41c0a27dcf041902e9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d66cfb5e14441a2a7044af58f3ac249",
            "placeholder": "​",
            "style": "IPY_MODEL_3eada2b6e16d410d86af9e55f0dfa294",
            "value": "Connecting..."
          }
        },
        "3d66cfb5e14441a2a7044af58f3ac249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eada2b6e16d410d86af9e55f0dfa294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krdlEfx6fZrW",
        "outputId": "f8a4bebe-0c72-449b-a272-1f75d888df32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score"
      ],
      "metadata": {
        "id": "FqBNkCJmfvJT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "108326f3fe5c44489f58d0c3e7fcb73f",
            "6a7b0bb73f6d4ff5b56a318685b8f7ff",
            "fa91ba7cfc76465484e50249ce847858",
            "1fa002466c9c464b88c745745a51fdc5",
            "01dae3ce630b47a89e69ba1c6d28d414",
            "78d50014f7db4d08bafdc02d63b07809",
            "1b7d9e322d0d4de8a7f9a8cc83207b5a",
            "98eeb628db0845a8a791e6346cde2474",
            "0b38b3a399c0403d9085861162b3016a",
            "9d44d4964fd647999cb3a54f415be8e2",
            "5fea14fdf19a422493ecba7c7b8a0d81",
            "3b7d8cf57f4a4b6e840ffb1d7940ef03",
            "fbc0d876b99e4346b71626e8af51fef3",
            "2ec7e75e17b641b7a5117c8ab3e4bcbc",
            "12ac7ec9d4b84ce797c9faffc88bbd1c",
            "7ae53229323847d7bc0e21fee0867cfc",
            "8a5e3aaf532f46cfb8cad69bdb3c1924",
            "3d71d259271e41c0a27dcf041902e9a8",
            "3d66cfb5e14441a2a7044af58f3ac249",
            "3eada2b6e16d410d86af9e55f0dfa294"
          ]
        },
        "id": "o3Ve6QvxfvQL",
        "outputId": "4cec6cc1-a494-4515-8821-ea7da2736425"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "108326f3fe5c44489f58d0c3e7fcb73f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "# Define model parameters\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "MAX_NEW_TOKENS = 5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(f\"Loading model {MODEL_ID} on {DEVICE}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "imdb = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UOw3ZD7fvbh",
        "outputId": "fbded2b9-8e13-4136-a9b6-10d50f836528"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model meta-llama/Llama-3.2-1B-Instruct on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare shots for few-shot prompting\n",
        "def prepare_examples(dataset, num_examples=5):\n",
        "    \"\"\"Prepare examples for few-shot prompting.\"\"\"\n",
        "    # Get balanced examples (equal number of positive and negative)\n",
        "    pos_examples = [item for item in dataset if item[\"label\"] == 1][:num_examples//2]\n",
        "    neg_examples = [item for item in dataset if item[\"label\"] == 0][:num_examples//2]\n",
        "\n",
        "    # If num_examples is odd, we add one more positive example\n",
        "    if num_examples % 2 == 1 and num_examples > 0:\n",
        "        pos_examples.append(dataset[dataset.index(random.choice([item for item in dataset if item[\"label\"] == 1]))])\n",
        "\n",
        "    examples = pos_examples + neg_examples\n",
        "    random.shuffle(examples)\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Format examples for few-shot prompting\n",
        "def format_few_shot_examples(examples):\n",
        "    formatted_examples = \"\"\n",
        "    for example in examples:\n",
        "        sentiment = \"positive\" if example[\"label\"] == 1 else \"negative\"\n",
        "        formatted_examples += f\"Review: {example['text']}\\nSentiment: {sentiment}\\n\\n\"\n",
        "    return formatted_examples\n",
        "\n",
        "# Create prompts for different numbers of shots\n",
        "def create_prompt(text, examples, shot_count):\n",
        "    system_prompt = (\n",
        "        \"You are a sentiment analysis expert. Given a movie review, classify the sentiment as either positive or negative. \"\n",
        "        \"Only output 'positive' or 'negative' without any other text or explanation.\"\n",
        "    )\n",
        "\n",
        "    if shot_count == 0:\n",
        "        prompt = f\"{system_prompt}\\n\\nReview: {text}\\nSentiment:\"\n",
        "    else:\n",
        "        few_shot_examples = format_few_shot_examples(examples[:shot_count])\n",
        "        prompt = f\"{system_prompt}\\n\\n{few_shot_examples}Review: {text}\\nSentiment:\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Function for inference\n",
        "def predict_sentiment(prompt, text, examples, shot_count, max_retries=2):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    retries = 0\n",
        "    while retries <= max_retries:\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False,\n",
        "                temperature=0.1,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = full_response[len(prompt):]\n",
        "\n",
        "        # Clean up the prediction - strip whitespace and lowercase\n",
        "        prediction = prediction.lower().strip()\n",
        "\n",
        "        # Check if prediction starts with either \"positive\" or \"negative\"\n",
        "        if prediction.startswith(\"positive\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif prediction.startswith(\"negative\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        # Try again with partial matches at the start\n",
        "        first_word = prediction.split()[0] if prediction.split() else \"\"\n",
        "        if first_word == \"positive\" or first_word.startswith(\"pos\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif first_word == \"negative\" or first_word.startswith(\"neg\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        # If this is our last retry, make a simple default choice\n",
        "        if retries == max_retries:\n",
        "            # Return the raw prediction and default to the opposite of the true label\n",
        "            return prediction, -1, retries\n",
        "\n",
        "        # Otherwise, create a stronger prompt and retry\n",
        "        retries += 1\n",
        "\n",
        "        # Create a stronger prompt - be very explicit about the first word\n",
        "        system_prompt = (\n",
        "            \"You are a sentiment analysis expert. Given a movie review, classify the sentiment as either positive or negative. \"\n",
        "            \"CRITICAL INSTRUCTION: Your response MUST START with EXACTLY the word 'positive' or the word 'negative'. \"\n",
        "            \"The first word of your response must be either 'positive' or 'negative', representing the answer.\"\n",
        "        )\n",
        "\n",
        "        if shot_count == 0:\n",
        "            prompt = f\"{system_prompt}\\n\\nReview: {text}\\nSentiment:\"\n",
        "        else:\n",
        "            few_shot_examples = format_few_shot_examples(examples[:shot_count])\n",
        "            prompt = f\"{system_prompt}\\n\\n{few_shot_examples}Review: {text}\\nSentiment:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)"
      ],
      "metadata": {
        "id": "h4Fact4KfviC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model\n",
        "def evaluate_model(dataset, shots, num_samples=200):\n",
        "    results = {\n",
        "        \"shots\": shots,\n",
        "        \"accuracy\": [],\n",
        "        \"f1_score\": [],\n",
        "        \"positive_precision\": [],\n",
        "        \"negative_precision\": [],\n",
        "        \"positive_recall\": [],\n",
        "        \"negative_recall\": [],\n",
        "        \"avg_retries\": [],\n",
        "        # Add these new metrics\n",
        "        \"corrected_accuracy\": [],\n",
        "        \"corrected_f1_score\": []\n",
        "    }\n",
        "\n",
        "    # Sample a subset from the test set for evaluation\n",
        "    print(f\"Sampling {num_samples} examples from the test set...\")\n",
        "    test_sample = random.sample(list(dataset), num_samples)\n",
        "\n",
        "    # Get examples from training set for few-shot prompting\n",
        "    train_examples = prepare_examples(list(imdb[\"train\"]), max(shots))\n",
        "    print(f\"Selected {max(shots)} examples from training set for few-shot prompting\")\n",
        "\n",
        "    # Save predictions for analysis\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Track retry statistics\n",
        "    retry_stats = []\n",
        "\n",
        "    for shot_count in shots:\n",
        "        print(f\"\\nEvaluating with {shot_count}-shot prompting...\")\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        # Lists to track corrected predictions (ignoring max retries)\n",
        "        corrected_predictions = []\n",
        "        corrected_true_labels = []\n",
        "\n",
        "        for item in tqdm(test_sample):\n",
        "            prompt = create_prompt(item[\"text\"], train_examples, shot_count)\n",
        "            prediction, pred_label, retry_count = predict_sentiment(prompt, item[\"text\"], train_examples, shot_count)\n",
        "\n",
        "            # Track retry statistics\n",
        "            if retry_count > 0:\n",
        "                retry_stats.append({\n",
        "                    \"shots\": shot_count,\n",
        "                    \"retries\": retry_count,\n",
        "                    \"text_preview\": item[\"text\"][:100] + \"...\",\n",
        "                    \"final_prediction\": prediction\n",
        "                })\n",
        "\n",
        "            # For standard metrics (same as before)\n",
        "            # If model still couldn't give a clear answer after retries, count it as wrong\n",
        "            if pred_label == -1:\n",
        "                # Assign a label opposite to the true label to ensure it's counted as wrong\n",
        "                pred_label = 1 - item[\"label\"]\n",
        "                predictions.append(pred_label)\n",
        "                true_labels.append(item[\"label\"])\n",
        "            else:\n",
        "                predictions.append(pred_label)\n",
        "                true_labels.append(item[\"label\"])\n",
        "\n",
        "                # For corrected metrics, only include if we got a valid prediction\n",
        "                # (i.e., didn't need too many retries)\n",
        "                corrected_predictions.append(pred_label)\n",
        "                corrected_true_labels.append(item[\"label\"])\n",
        "\n",
        "            # Store for later analysis\n",
        "            all_predictions.append({\n",
        "                \"shots\": shot_count,\n",
        "                \"text\": item[\"text\"][:100] + \"...\",\n",
        "                \"true_label\": \"positive\" if item[\"label\"] == 1 else \"negative\",\n",
        "                \"predicted\": prediction,\n",
        "                \"correct\": pred_label == item[\"label\"],\n",
        "                \"retries\": retry_count,\n",
        "                \"exceeded_max_retries\": pred_label == -1\n",
        "            })\n",
        "\n",
        "        # Calculate standard metrics\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        report = classification_report(true_labels, predictions, target_names=[\"negative\", \"positive\"], output_dict=True)\n",
        "\n",
        "        # Calculate corrected metrics\n",
        "        corrected_accuracy = accuracy_score(corrected_true_labels, corrected_predictions) if corrected_true_labels else 0\n",
        "        corrected_f1 = f1_score(corrected_true_labels, corrected_predictions, average='weighted') if corrected_true_labels else 0\n",
        "\n",
        "        # Calculate average retries for this shot count\n",
        "        retry_counts_for_shot = [r[\"retries\"] for r in retry_stats if r[\"shots\"] == shot_count]\n",
        "        avg_retries = sum(retry_counts_for_shot) / len(test_sample) if retry_counts_for_shot else 0\n",
        "\n",
        "        # Store all results\n",
        "        results[\"accuracy\"].append(accuracy)\n",
        "        results[\"f1_score\"].append(f1)\n",
        "        results[\"positive_precision\"].append(report[\"positive\"][\"precision\"])\n",
        "        results[\"negative_precision\"].append(report[\"negative\"][\"precision\"])\n",
        "        results[\"positive_recall\"].append(report[\"positive\"][\"recall\"])\n",
        "        results[\"negative_recall\"].append(report[\"negative\"][\"recall\"])\n",
        "        results[\"avg_retries\"].append(avg_retries)\n",
        "\n",
        "        # Store corrected metrics\n",
        "        results[\"corrected_accuracy\"].append(corrected_accuracy)\n",
        "        results[\"corrected_f1_score\"].append(corrected_f1)\n",
        "\n",
        "        # Include the corrected metrics in the output\n",
        "        print(f\"Accuracy with {shot_count}-shot: {accuracy:.4f}, F1: {f1:.4f} (avg retries: {avg_retries:.2f})\")\n",
        "        print(f\"Corrected metrics (ignoring max retries): Accuracy: {corrected_accuracy:.4f}, F1: {corrected_f1:.4f}\")\n",
        "        print(f\"Valid samples: {len(corrected_true_labels)}/{len(test_sample)} ({len(corrected_true_labels)/len(test_sample)*100:.1f}%)\")\n",
        "\n",
        "    # Save predictions for analysis\n",
        "    pd.DataFrame(all_predictions).to_csv(\"sentiment_predictions.csv\", index=False)\n",
        "\n",
        "    # Save retry statistics\n",
        "    if retry_stats:\n",
        "        pd.DataFrame(retry_stats).to_csv(\"retry_statistics.csv\", index=False)\n",
        "        print(f\"Total retries needed: {len(retry_stats)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Define shot counts to evaluate\n",
        "shots = [0, 1, 3, 5, 10, 20]"
      ],
      "metadata": {
        "id": "ZcaMuS73fvkb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000"
      ],
      "metadata": {
        "id": "Xmf5VqeMfvml"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation using train set for examples and test set for evaluation\n",
        "print(\"Getting few-shot examples from training set...\")\n",
        "# Run evaluation\n",
        "results = evaluate_model(imdb[\"test\"], shots, num_samples)\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    \"Shots\": results[\"shots\"],\n",
        "    \"Accuracy\": results[\"accuracy\"],\n",
        "    \"Corrected Accuracy\": results[\"corrected_accuracy\"],\n",
        "    \"F1 Score\": results[\"f1_score\"],\n",
        "    \"Corrected F1 Score\": results[\"corrected_f1_score\"],\n",
        "    \"Avg Retries\": results[\"avg_retries\"],\n",
        "    \"Positive Precision\": results[\"positive_precision\"],\n",
        "    \"Negative Precision\": results[\"negative_precision\"],\n",
        "    \"Positive Recall\": results[\"positive_recall\"],\n",
        "    \"Negative Recall\": results[\"negative_recall\"]\n",
        "})\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(\"sentiment_analysis_results.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iecWh_n-fvov",
        "outputId": "517b86ad-8a4f-4c6b-9273-185b2ca23e1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting few-shot examples from training set...\n",
            "Sampling 1000 examples from the test set...\n",
            "Selected 20 examples from training set for few-shot prompting\n",
            "\n",
            "Evaluating with 0-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [01:37<00:00, 10.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 0-shot: 0.8770, F1: 0.8769 (avg retries: 0.09)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.9164, F1: 0.9164\n",
            "Valid samples: 957/1000 (95.7%)\n",
            "\n",
            "Evaluating with 1-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [01:57<00:00,  8.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 1-shot: 0.8700, F1: 0.8696 (avg retries: 0.09)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.9081, F1: 0.9078\n",
            "Valid samples: 958/1000 (95.8%)\n",
            "\n",
            "Evaluating with 3-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [02:24<00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 3-shot: 0.8320, F1: 0.8308 (avg retries: 0.09)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.8694, F1: 0.8684\n",
            "Valid samples: 957/1000 (95.7%)\n",
            "\n",
            "Evaluating with 5-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [03:29<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 5-shot: 0.8540, F1: 0.8535 (avg retries: 0.09)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.8924, F1: 0.8920\n",
            "Valid samples: 957/1000 (95.7%)\n",
            "\n",
            "Evaluating with 10-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [05:44<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10-shot: 0.8780, F1: 0.8779 (avg retries: 0.09)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.9175, F1: 0.9174\n",
            "Valid samples: 957/1000 (95.7%)\n",
            "\n",
            "Evaluating with 20-shot prompting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [31:27<00:00,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 20-shot: 0.0000, F1: 0.0000 (avg retries: 2.00)\n",
            "Corrected metrics (ignoring max retries): Accuracy: 0.0000, F1: 0.0000\n",
            "Valid samples: 0/1000 (0.0%)\n",
            "Total retries needed: 1215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results table\n",
        "print(\"\\nResults Summary:\")\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNbY9LrMqUbD",
        "outputId": "222a248e-f819-4d40-f629-127da686647f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results Summary:\n",
            " Shots  Accuracy  Corrected Accuracy  F1 Score  Corrected F1 Score  Avg Retries  Positive Precision  Negative Precision  Positive Recall  Negative Recall\n",
            "     0     0.877            0.916405  0.876923            0.916351        0.086            0.859048            0.896842            0.902            0.852\n",
            "     1     0.870            0.908142  0.869620            0.907849        0.085            0.914798            0.833935            0.816            0.924\n",
            "     3     0.832            0.869383  0.830806            0.868381        0.086            0.784247            0.899038            0.916            0.748\n",
            "     5     0.854            0.892372  0.853507            0.891982        0.086            0.817204            0.900452            0.912            0.796\n",
            "    10     0.878            0.917450  0.877875            0.917361        0.086            0.855263            0.903846            0.910            0.846\n",
            "    20     0.000            0.000000  0.000000            0.000000        2.000            0.000000            0.000000            0.000            0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the 10-shot and 20-shot doesn't format it properly, but gives 'ositive' and 'egative' as answers (based on debugging statements that were removed to improve readability). For the sake of experimentation, let's just test out what its accuracy is if these can be taken as correct answers. Let's start with the 20-shot one"
      ],
      "metadata": {
        "id": "s3bPRPrUyR__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified prediction function to allow partial matches like 'ositive' and 'egative'\n",
        "def predict_sentiment_flexible(prompt, text, examples, shot_count=20, max_retries=2):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    retries = 0\n",
        "    while retries <= max_retries:\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False,\n",
        "                temperature=0.1,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = full_response[len(prompt):]\n",
        "\n",
        "        # Clean up the prediction - strip whitespace and lowercase\n",
        "        prediction = prediction.lower().strip()\n",
        "\n",
        "        # Now checking for standard or partial matches\n",
        "        if prediction.startswith(\"positive\") or prediction.startswith(\"ositive\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif prediction.startswith(\"negative\") or prediction.startswith(\"egative\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        first_word = prediction.split()[0] if prediction.split() else \"\"\n",
        "        if first_word in [\"positive\", \"ositive\"] or first_word.startswith(\"pos\") or first_word.startswith(\"osi\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif first_word in [\"negative\", \"egative\"] or first_word.startswith(\"neg\") or first_word.startswith(\"ega\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        if retries == max_retries:\n",
        "            return prediction, -1, retries\n",
        "\n",
        "        # Otherwise, create a stronger prompt and retry\n",
        "        retries += 1\n",
        "\n",
        "        # Create a stronger prompt with more flexible matching instructions\n",
        "        system_prompt = (\n",
        "            \"You are a sentiment analysis expert. Given a movie review, classify the sentiment as either positive or negative. \"\n",
        "            \"CRITICAL INSTRUCTION: Your response MUST START with EITHER 'positive' OR 'negative' (or just 'ositive'/'egative'). \"\n",
        "            \"Only output the sentiment classification without any explanation.\"\n",
        "        )\n",
        "\n",
        "        if shot_count == 0:\n",
        "            prompt = f\"{system_prompt}\\n\\nReview: {text}\\nSentiment:\"\n",
        "        else:\n",
        "            few_shot_examples = format_few_shot_examples(examples[:shot_count])\n",
        "            prompt = f\"{system_prompt}\\n\\n{few_shot_examples}Review: {text}\\nSentiment:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Run 20-shot evaluation using the flexible prediction function\n",
        "def run_20_shot_evaluation(num_samples=50):\n",
        "    print(f\"Running 20-shot evaluation on {num_samples} test samples with flexible matching...\")\n",
        "\n",
        "    # Get examples from training set for 20-shot prompting\n",
        "    train_examples = prepare_examples(list(imdb[\"train\"]), 20)\n",
        "\n",
        "    # Sample from test set\n",
        "    test_sample = random.sample(list(imdb[\"test\"]), num_samples)\n",
        "\n",
        "    # Track results\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    # Add corrected predictions tracking\n",
        "    corrected_predictions = []\n",
        "    corrected_true_labels = []\n",
        "    details = []\n",
        "    retry_counts = []\n",
        "\n",
        "    for item in tqdm(test_sample):\n",
        "        prompt = create_prompt(item[\"text\"], train_examples, 20)\n",
        "        prediction, pred_label, retry_count = predict_sentiment_flexible(prompt, item[\"text\"], train_examples, 20)\n",
        "\n",
        "        # If model still couldn't give a clear answer after retries, count it as wrong\n",
        "        if pred_label == -1:\n",
        "            # Assign a label opposite to the true label to ensure it's counted as wrong\n",
        "            pred_label = 1 - item[\"label\"]\n",
        "            predictions.append(pred_label)\n",
        "            true_labels.append(item[\"label\"])\n",
        "        else:\n",
        "            predictions.append(pred_label)\n",
        "            true_labels.append(item[\"label\"])\n",
        "\n",
        "            # For corrected metrics, only include valid predictions\n",
        "            corrected_predictions.append(pred_label)\n",
        "            corrected_true_labels.append(item[\"label\"])\n",
        "\n",
        "        retry_counts.append(retry_count)\n",
        "\n",
        "        # Store details for analysis\n",
        "        details.append({\n",
        "            \"text_preview\": item[\"text\"][:100] + \"...\",\n",
        "            \"true_label\": \"positive\" if item[\"label\"] == 1 else \"negative\",\n",
        "            \"predicted\": prediction,\n",
        "            \"correct\": pred_label == item[\"label\"],\n",
        "            \"retries\": retry_count,\n",
        "            \"exceeded_max_retries\": pred_label == -1\n",
        "        })\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "    # Calculate corrected metrics\n",
        "    corrected_accuracy = accuracy_score(corrected_true_labels, corrected_predictions) if corrected_true_labels else 0\n",
        "    corrected_f1 = f1_score(corrected_true_labels, corrected_predictions, average='weighted') if corrected_true_labels else 0\n",
        "\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"negative\", \"positive\"], output_dict=True)\n",
        "    avg_retries = sum(retry_counts) / len(retry_counts)\n",
        "\n",
        "    print(f\"\\n--- 20-Shot Results with Flexible Matching ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Corrected accuracy: {corrected_accuracy:.4f}\")\n",
        "    print(f\"Corrected F1 score: {corrected_f1:.4f}\")\n",
        "    print(f\"Valid samples: {len(corrected_true_labels)}/{len(test_sample)} ({len(corrected_true_labels)/len(test_sample)*100:.1f}%)\")\n",
        "    print(f\"Average retries: {avg_retries:.2f}\")\n",
        "    print(f\"Positive precision: {report['positive']['precision']:.4f}\")\n",
        "    print(f\"Negative precision: {report['negative']['precision']:.4f}\")\n",
        "    print(f\"Positive recall: {report['positive']['recall']:.4f}\")\n",
        "    print(f\"Negative recall: {report['negative']['recall']:.4f}\")\n",
        "\n",
        "    # Return results dict and details for further analysis\n",
        "    results = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,\n",
        "        \"corrected_accuracy\": corrected_accuracy,\n",
        "        \"corrected_f1_score\": corrected_f1,\n",
        "        \"avg_retries\": avg_retries,\n",
        "        \"report\": report,\n",
        "        \"details\": details\n",
        "    }\n",
        "\n",
        "    # Save detailed results to CSV\n",
        "    pd.DataFrame(details).to_csv(\"flexible_20shot_results.csv\", index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute the 20-shot evaluation\n",
        "results_20shot = run_20_shot_evaluation(num_samples=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTv2c5CXyRq2",
        "outputId": "72fd44a8-8fa2-40ca-adb1-a8d0eebafff9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 20-shot evaluation on 1000 test samples with flexible matching...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [12:29<00:00,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 20-Shot Results with Flexible Matching ---\n",
            "Accuracy: 0.7580\n",
            "F1 Score: 0.7551\n",
            "Corrected accuracy: 0.8376\n",
            "Corrected F1 score: 0.8350\n",
            "Valid samples: 905/1000 (90.5%)\n",
            "Average retries: 0.19\n",
            "Positive precision: 0.8560\n",
            "Negative precision: 0.6956\n",
            "Positive recall: 0.6416\n",
            "Negative recall: 0.8836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets try with the 10-shot one"
      ],
      "metadata": {
        "id": "uereI-TtCdGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified prediction function to allow partial matches like 'ositive' and 'egative'\n",
        "def predict_sentiment_flexible(prompt, text, examples, shot_count=10, max_retries=2):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    retries = 0\n",
        "    while retries <= max_retries:\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False,\n",
        "                temperature=0.1,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = full_response[len(prompt):]\n",
        "\n",
        "        # Clean up the prediction - strip whitespace and lowercase\n",
        "        prediction = prediction.lower().strip()\n",
        "\n",
        "        # Now checking for standard or partial matches\n",
        "        if prediction.startswith(\"positive\") or prediction.startswith(\"ositive\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif prediction.startswith(\"negative\") or prediction.startswith(\"egative\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        first_word = prediction.split()[0] if prediction.split() else \"\"\n",
        "        if first_word in [\"positive\", \"ositive\"] or first_word.startswith(\"pos\") or first_word.startswith(\"osi\"):\n",
        "            return \"positive\", 1, retries\n",
        "        elif first_word in [\"negative\", \"egative\"] or first_word.startswith(\"neg\") or first_word.startswith(\"ega\"):\n",
        "            return \"negative\", 0, retries\n",
        "\n",
        "        if retries == max_retries:\n",
        "            return prediction, -1, retries\n",
        "\n",
        "        # Otherwise, create a stronger prompt and retry\n",
        "        retries += 1\n",
        "\n",
        "        # Create a stronger prompt with more flexible matching instructions\n",
        "        system_prompt = (\n",
        "            \"You are a sentiment analysis expert. Given a movie review, classify the sentiment as either positive or negative. \"\n",
        "            \"CRITICAL INSTRUCTION: Your response MUST START with EITHER 'positive' OR 'negative' (or just 'ositive'/'egative'). \"\n",
        "            \"Only output the sentiment classification without any explanation.\"\n",
        "        )\n",
        "\n",
        "        if shot_count == 0:\n",
        "            prompt = f\"{system_prompt}\\n\\nReview: {text}\\nSentiment:\"\n",
        "        else:\n",
        "            few_shot_examples = format_few_shot_examples(examples[:shot_count])\n",
        "            prompt = f\"{system_prompt}\\n\\n{few_shot_examples}Review: {text}\\nSentiment:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Run 10-shot evaluation using the flexible prediction function\n",
        "def run_10_shot_evaluation(num_samples=50):\n",
        "    print(f\"Running 10-shot evaluation on {num_samples} test samples with flexible matching...\")\n",
        "\n",
        "    # Get examples from training set for 10-shot prompting\n",
        "    train_examples = prepare_examples(list(imdb[\"train\"]), 10)\n",
        "\n",
        "    # Sample from test set\n",
        "    test_sample = random.sample(list(imdb[\"test\"]), num_samples)\n",
        "\n",
        "    # Track results\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    # Add corrected predictions tracking\n",
        "    corrected_predictions = []\n",
        "    corrected_true_labels = []\n",
        "    details = []\n",
        "    retry_counts = []\n",
        "\n",
        "    for item in tqdm(test_sample):\n",
        "        prompt = create_prompt(item[\"text\"], train_examples, 10)\n",
        "        prediction, pred_label, retry_count = predict_sentiment_flexible(prompt, item[\"text\"], train_examples, 10)\n",
        "\n",
        "        # If model still couldn't give a clear answer after retries, count it as wrong\n",
        "        if pred_label == -1:\n",
        "            # Assign a label opposite to the true label to ensure it's counted as wrong\n",
        "            pred_label = 1 - item[\"label\"]\n",
        "            predictions.append(pred_label)\n",
        "            true_labels.append(item[\"label\"])\n",
        "        else:\n",
        "            predictions.append(pred_label)\n",
        "            true_labels.append(item[\"label\"])\n",
        "\n",
        "            # For corrected metrics, only include valid predictions\n",
        "            corrected_predictions.append(pred_label)\n",
        "            corrected_true_labels.append(item[\"label\"])\n",
        "\n",
        "        retry_counts.append(retry_count)\n",
        "\n",
        "        # Store details for analysis\n",
        "        details.append({\n",
        "            \"text_preview\": item[\"text\"][:100] + \"...\",\n",
        "            \"true_label\": \"positive\" if item[\"label\"] == 1 else \"negative\",\n",
        "            \"predicted\": prediction,\n",
        "            \"correct\": pred_label == item[\"label\"],\n",
        "            \"retries\": retry_count,\n",
        "            \"exceeded_max_retries\": pred_label == -1\n",
        "        })\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "    # Calculate corrected metrics\n",
        "    corrected_accuracy = accuracy_score(corrected_true_labels, corrected_predictions) if corrected_true_labels else 0\n",
        "    corrected_f1 = f1_score(corrected_true_labels, corrected_predictions, average='weighted') if corrected_true_labels else 0\n",
        "\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"negative\", \"positive\"], output_dict=True)\n",
        "    avg_retries = sum(retry_counts) / len(retry_counts)\n",
        "\n",
        "    print(f\"\\n--- 10-Shot Results with Flexible Matching ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Corrected accuracy: {corrected_accuracy:.4f}\")\n",
        "    print(f\"Corrected F1 score: {corrected_f1:.4f}\")\n",
        "    print(f\"Valid samples: {len(corrected_true_labels)}/{len(test_sample)} ({len(corrected_true_labels)/len(test_sample)*100:.1f}%)\")\n",
        "    print(f\"Average retries: {avg_retries:.2f}\")\n",
        "    print(f\"Positive precision: {report['positive']['precision']:.4f}\")\n",
        "    print(f\"Negative precision: {report['negative']['precision']:.4f}\")\n",
        "    print(f\"Positive recall: {report['positive']['recall']:.4f}\")\n",
        "    print(f\"Negative recall: {report['negative']['recall']:.4f}\")\n",
        "\n",
        "    # Return results dict and details for further analysis\n",
        "    results = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,\n",
        "        \"corrected_accuracy\": corrected_accuracy,\n",
        "        \"corrected_f1_score\": corrected_f1,\n",
        "        \"avg_retries\": avg_retries,\n",
        "        \"report\": report,\n",
        "        \"details\": details\n",
        "    }\n",
        "\n",
        "    # Save detailed results to CSV\n",
        "    pd.DataFrame(details).to_csv(\"flexible_10shot_results.csv\", index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute the 10-shot evaluation\n",
        "results_10shot = run_10_shot_evaluation(num_samples=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYNsLi3ICCXh",
        "outputId": "b01f4fd3-0832-4f3a-caf7-70a5e8d9a320"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 10-shot evaluation on 1000 test samples with flexible matching...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1000/1000 [06:36<00:00,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 10-Shot Results with Flexible Matching ---\n",
            "Accuracy: 0.8390\n",
            "F1 Score: 0.8388\n",
            "Corrected accuracy: 0.9374\n",
            "Corrected F1 score: 0.9374\n",
            "Valid samples: 895/1000 (89.5%)\n",
            "Average retries: 0.21\n",
            "Positive precision: 0.8241\n",
            "Negative precision: 0.8565\n",
            "Positive recall: 0.8708\n",
            "Negative recall: 0.8057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}