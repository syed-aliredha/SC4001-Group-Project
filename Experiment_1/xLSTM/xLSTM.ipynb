{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "5w_xZihHU-0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQ-YCKOQUuC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68809adc-6ad1-4f95-da71-0132baf42e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlstm in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from xlstm) (3.4.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.3.0)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.3.1)\n",
            "Requirement already satisfied: joypy in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.2.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from xlstm) (6.17.1)\n",
            "Requirement already satisfied: dacite in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.9.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from xlstm) (6.3.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.11.1.4)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.30.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from xlstm) (13.9.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.67.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.13.2)\n",
            "Requirement already satisfied: mlstm_kernels in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->xlstm) (0.2.13)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (5.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scipy>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->xlstm) (4.9.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (11.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (5.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (2.18.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->xlstm) (5.7.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->xlstm) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->xlstm) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->xlstm) (4.3.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->xlstm) (0.7.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install xlstm transformers datasets torch\n",
        "!apt-get install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocess Data"
      ],
      "metadata": {
        "id": "0AqY1NS-vke7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "transformers.set_seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 256\n",
        "NUM_BLOCKS = 2\n",
        "NUM_HEADS = 4\n",
        "EPOCHS = 20\n",
        "EMBED_DIM = 128\n",
        "NUM_CLASSES = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.texts = dataset[split]['text']\n",
        "        self.labels = dataset[split]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoded['input_ids'].squeeze(0)\n",
        "        return input_ids, torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "train_data = IMDBDataset('train')\n",
        "test_data = IMDBDataset('test')\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(len(train_loader.dataset.labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpY_NgfwikBz",
        "outputId": "bb9b417b-b8eb-4998-e5cf-a7c1f54cf794"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from xlstm.xlstm_large.model import xLSTMLargeConfig, xLSTMLarge\n",
        "\n",
        "# Define model configuration\n",
        "config = xLSTMLargeConfig(\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    return_last_states=False,\n",
        "    mode=\"inference\",\n",
        "    chunkwise_kernel=\"chunkwise--native_autograd\",\n",
        "    sequence_kernel=\"native_sequence__native\",\n",
        "    step_kernel=\"native\",\n",
        ")\n",
        "\n",
        "class xLSTMClassifier(nn.Module):\n",
        "    def __init__(self, config, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.backbone = xLSTMLarge(config)\n",
        "\n",
        "        if hasattr(self.backbone, \"lm_head\"):\n",
        "            del self.backbone.lm_head\n",
        "\n",
        "        self.backbone.lm_head = nn.Linear(EMBED_DIM, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        if x.ndim == 3: x = x[:, -1, :]\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = xLSTMClassifier(config).to(device)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "wp5GIbPxinqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa3b0fa-e951-415f-a395-078cfcd48b6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "xLSTMClassifier(\n",
              "  (backbone): xLSTMLarge(\n",
              "    (embedding): Embedding(30522, 128)\n",
              "    (backbone): xLSTMLargeBlockStack(\n",
              "      (blocks): ModuleList(\n",
              "        (0-1): 2 x mLSTMBlock(\n",
              "          (norm_mlstm): RMSNorm()\n",
              "          (mlstm_layer): mLSTMLayer(\n",
              "            (q): Linear(in_features=128, out_features=64, bias=False)\n",
              "            (k): Linear(in_features=128, out_features=64, bias=False)\n",
              "            (v): Linear(in_features=128, out_features=128, bias=False)\n",
              "            (ogate_preact): Linear(in_features=128, out_features=128, bias=False)\n",
              "            (igate_preact): Linear(in_features=128, out_features=4, bias=True)\n",
              "            (fgate_preact): Linear(in_features=128, out_features=4, bias=True)\n",
              "            (ogate_act_fn): Sigmoid()\n",
              "            (mlstm_backend): mLSTMBackend(mLSTMBackendConfig(chunkwise_kernel='chunkwise--native_autograd', sequence_kernel='native_sequence__native', step_kernel='native', mode='inference', chunk_size=64, return_last_states=False, autocast_kernel_dtype='bfloat16', eps=1e-06, inference_state_dtype='float32'))\n",
              "            (multihead_norm): MultiHeadLayerNorm()\n",
              "            (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
              "          )\n",
              "          (norm_ffn): RMSNorm()\n",
              "          (ffn): FeedForward(\n",
              "            (proj_up_gate): Linear(in_features=128, out_features=384, bias=False)\n",
              "            (proj_up): Linear(in_features=128, out_features=384, bias=False)\n",
              "            (proj_down): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (out_norm): RMSNorm()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=128, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Number of trainable parameters:\", pytorch_total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_tJmmQd4P3h",
        "outputId": "25dca622-9128-47d6-b9b2-3b750301026e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 4336018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Results"
      ],
      "metadata": {
        "id": "PVFK0VZ3PcMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import time\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2, betas=(0.9, 0.99), weight_decay=0.1)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total, correct = 0, 0\n",
        "    start = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = outputs.argmax(1)\n",
        "        correct += (pred == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    print(f\"Train Acc: {correct/total*100:.2f}% | Time: {time.time() - start:.1f}s\")\n",
        "\n",
        "\n",
        "def evaluate(best_acc, patience):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            pred = outputs.argmax(1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = correct/total\n",
        "    print(f\"Test Acc: {acc*100:.2f}%\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        patience = 5\n",
        "    else:\n",
        "        patience -= 1\n",
        "    return best_acc, patience\n",
        "\n",
        "best_acc = 0\n",
        "patience = 5\n",
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    train()\n",
        "    best_acc, patience = evaluate(best_acc, patience)\n",
        "\n",
        "    if patience == 0:\n",
        "      print(\"Early stopping!\")\n",
        "      break\n"
      ],
      "metadata": {
        "id": "EvExs0Riiv9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b7ed13-ac3e-4c2d-8491-f19c79b88039"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "Train Acc: 50.05% | Time: 61.7s\n",
            "Test Acc: 50.00%\n",
            "\n",
            "Epoch 2/30\n",
            "Train Acc: 49.12% | Time: 61.2s\n",
            "Test Acc: 50.00%\n",
            "\n",
            "Epoch 3/30\n",
            "Train Acc: 49.35% | Time: 60.5s\n",
            "Test Acc: 50.00%\n",
            "\n",
            "Epoch 4/30\n",
            "Train Acc: 49.84% | Time: 60.6s\n",
            "Test Acc: 49.22%\n",
            "\n",
            "Epoch 5/30\n",
            "Train Acc: 50.22% | Time: 60.2s\n",
            "Test Acc: 50.00%\n",
            "\n",
            "Epoch 6/30\n",
            "Train Acc: 50.78% | Time: 60.3s\n",
            "Test Acc: 50.01%\n",
            "\n",
            "Epoch 7/30\n",
            "Train Acc: 51.24% | Time: 60.3s\n",
            "Test Acc: 50.00%\n",
            "\n",
            "Epoch 8/30\n",
            "Train Acc: 52.98% | Time: 60.3s\n",
            "Test Acc: 50.96%\n",
            "\n",
            "Epoch 9/30\n",
            "Train Acc: 54.59% | Time: 60.4s\n",
            "Test Acc: 51.17%\n",
            "\n",
            "Epoch 10/30\n",
            "Train Acc: 55.44% | Time: 60.2s\n",
            "Test Acc: 52.46%\n",
            "\n",
            "Epoch 11/30\n",
            "Train Acc: 58.15% | Time: 60.2s\n",
            "Test Acc: 52.58%\n",
            "\n",
            "Epoch 12/30\n",
            "Train Acc: 59.65% | Time: 60.2s\n",
            "Test Acc: 52.61%\n",
            "\n",
            "Epoch 13/30\n",
            "Train Acc: 61.74% | Time: 60.7s\n",
            "Test Acc: 53.05%\n",
            "\n",
            "Epoch 14/30\n",
            "Train Acc: 62.97% | Time: 60.1s\n",
            "Test Acc: 54.57%\n",
            "\n",
            "Epoch 15/30\n",
            "Train Acc: 63.59% | Time: 60.3s\n",
            "Test Acc: 53.22%\n",
            "\n",
            "Epoch 16/30\n",
            "Train Acc: 64.50% | Time: 60.2s\n",
            "Test Acc: 54.14%\n",
            "\n",
            "Epoch 17/30\n",
            "Train Acc: 64.79% | Time: 60.2s\n",
            "Test Acc: 54.66%\n",
            "\n",
            "Epoch 18/30\n",
            "Train Acc: 64.95% | Time: 60.0s\n",
            "Test Acc: 54.20%\n",
            "\n",
            "Epoch 19/30\n",
            "Train Acc: 77.64% | Time: 60.2s\n",
            "Test Acc: 80.66%\n",
            "\n",
            "Epoch 20/30\n",
            "Train Acc: 88.26% | Time: 60.8s\n",
            "Test Acc: 84.38%\n",
            "\n",
            "Epoch 21/30\n",
            "Train Acc: 91.21% | Time: 60.2s\n",
            "Test Acc: 83.03%\n",
            "\n",
            "Epoch 22/30\n",
            "Train Acc: 92.26% | Time: 60.3s\n",
            "Test Acc: 82.66%\n",
            "\n",
            "Epoch 23/30\n",
            "Train Acc: 93.27% | Time: 60.1s\n",
            "Test Acc: 81.87%\n",
            "\n",
            "Epoch 24/30\n",
            "Train Acc: 93.85% | Time: 60.3s\n",
            "Test Acc: 82.11%\n",
            "\n",
            "Epoch 25/30\n",
            "Train Acc: 93.98% | Time: 60.2s\n",
            "Test Acc: 81.96%\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best accuracy obtained: 84.38%"
      ],
      "metadata": {
        "id": "WzzT_8CXTb5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peak_memory = torch.cuda.max_memory_allocated() / (2 ** 20)\n",
        "print(f\"Peak memory usage: {peak_memory:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7TZ3Vmn2OVt",
        "outputId": "9ef80ec4-1b52-456f-94f0-93e9bd52a4df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak memory usage: 373.97 MB\n"
          ]
        }
      ]
    }
  ]
}