{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "1CRSm4OHVCmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf4NyoemGKCx"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install datasets transformers evaluate optuna sentence-transformers setfit\n",
        "!apt-get install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Preprocess data"
      ],
      "metadata": {
        "id": "c2eGszhwijyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tyVQSMTGNy4"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "print(imdb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import sample_dataset\n",
        "from collections import Counter\n",
        "\n",
        "train_split = imdb['train'].shuffle(seed=42)\n",
        "test_split = imdb['test'].shuffle(seed=42)\n",
        "\n",
        "val_dataset = train_split.select(range(3000))\n",
        "train_dataset = sample_dataset(train_split.select(range(3000, len(train_split))), label_column=\"label\", num_samples=8)\n",
        "test_dataset = test_split.select(range(3000))\n",
        "\n",
        "label_counts = Counter(train_dataset['label'])\n",
        "print(\"Label distribution for train\", label_counts)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(val_dataset))\n",
        "\n"
      ],
      "metadata": {
        "id": "s8A0bKuWsaDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "TZxqNw-Jim2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import optuna\n",
        "import evaluate\n",
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "from setfit import SetFitModel, Trainer, TrainingArguments\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def hp_space(trial: optuna.Trial):\n",
        "    return {\n",
        "        \"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-6, 1e-3, log=True),\n",
        "        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 4),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    }\n",
        "\n",
        "def model_init(trial):\n",
        "    return SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    metric=\"accuracy\",\n",
        ")\n",
        "\n",
        "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=5)\n",
        "print(best_run)"
      ],
      "metadata": {
        "id": "_ROvZy9pXcE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Results"
      ],
      "metadata": {
        "id": "le7Spqp7lBkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1\": f1_metric.compute(predictions=preds, references=labels)[\"f1\"],\n",
        "        \"recall\": recall_metric.compute(predictions=preds, references=labels)[\"recall\"],\n",
        "        \"precision\": precision_metric.compute(predictions=preds, references=labels)[\"precision\"],\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    metric=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "RvmQC4CNUKmM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}